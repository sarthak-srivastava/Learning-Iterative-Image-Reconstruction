{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sarthak-srivastava/Learning-Iterative-Image-Reconstruction/blob/master/NNFL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "duWV_bfxXbgZ"
   },
   "source": [
    "Link to the paper: http://www.ais.uni-bonn.de/behnke/papers/ijcai01.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUK7ySBFWdW3"
   },
   "outputs": [],
   "source": [
    "#Lets get started by loading our basic dependencies\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wm5VUmRSOflf"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwYpk88uWslH"
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "jsc5rrTzQLbd",
    "outputId": "9619b8e5-5b4a-46c5-e038-002b972ebdfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-71e12f4bac70>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/sarthak/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/sarthak/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/sarthak/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/sarthak/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/sarthak/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "I3J37AhQWu5l",
    "outputId": "4b9b4fb5-89af-42b8-c8ce-9595f132da65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = mnist.train.images[:55000,:]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "eTfWGyvGW61V",
    "outputId": "a5074e7d-b4dc-4343-e619-c6e3d878ecaf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's have a look at a random image\n",
    "RandomNum = random.randint(0,55000)\n",
    "image = x_train[RandomNum].reshape([28,28])\n",
    "plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YT2L7WiIwp9x",
    "outputId": "db30355c-2b0a-44c4-c9fa-31c5a2e380af"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "eWGKIeEUuHN-",
    "outputId": "6c44ade2-6328-4e84-808e-3022870261f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "x =[0]*55000\n",
    "image_label = [0]*55000\n",
    "image_train = [0]*55000\n",
    "for i in range(55000):\n",
    "  x[i] = x_train[i].reshape([28,28])\n",
    "  image_label[i] = x_train[i].reshape([784,1])\n",
    "  image_train[i] = resize(x[i], (14,14))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "C-kqpRRKysiu",
    "outputId": "56f1a0e0-6f1a-4bd0-8ea2-9031096dfb89"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_train[RandomNum])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxmupT_xzhrO"
   },
   "outputs": [],
   "source": [
    "for i in range(55000):\n",
    "  image_train[i] = image_train[i].reshape([196,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "laDogaoK0AZW"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "from stackrnn import StackRNNCell, StackLSTMCell, RecurrentWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# hyperparameter search on algorithmic patterns\n",
    "\n",
    "n_iterations =100\n",
    "n_min = 2\n",
    "n_max_train = 15\n",
    "n_max_val = 19\n",
    "n_max_test = 60\n",
    "n_img_per_iter = \n",
    "model =\"srnn\"\n",
    "hidden_sizes = [20, 40, 100]\n",
    "hidden_layers = [1]\n",
    "sgd_lrs = [0.1, 0.01, 0.001]\n",
    "n_stacks = [1, 2, 5, 10]\n",
    "ks = [1, 2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  1\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  1\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  2\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  2\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  5\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  5\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  10\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  10\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  1\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  1\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  2\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  2\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  5\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  5\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  10\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  10\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  1\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  1\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  2\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  2\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  5\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  5\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  10\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  20\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  10\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  1\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  1\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  2\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  2\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  5\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  5\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  10\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  10\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  1\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  1\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  2\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  2\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  5\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  5\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  10\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  10\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  1\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  1\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  2\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  2\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  5\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  5\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  10\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  40\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  10\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  1\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  1\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  2\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  2\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  5\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  5\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  10\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.1\n",
      " stacks:  10\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  1\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  1\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  2\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  2\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  5\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  5\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  10\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.01\n",
      " stacks:  10\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  1\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  1\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  2\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  2\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  5\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  5\n",
      " k:  2\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  10\n",
      " k:  1\n",
      "============================================================\n",
      "============================================================\n",
      " units:  100\n",
      " layers:  1\n",
      " lr:  0.001\n",
      " stacks:  10\n",
      " k:  2\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "for hidden_size, n_layers, sgd_lr, n_stack, k in \\\n",
    "    product(hidden_sizes, hidden_layers, sgd_lrs, n_stacks, ks):\n",
    "    print(\"=\" * 60)\n",
    "    print(\" units: \", hidden_size)\n",
    "    print(\" layers: \", n_layers)\n",
    "    print(\" lr: \", sgd_lr)\n",
    "    print(\" stacks: \", n_stack)\n",
    "    print(\" k: \", k)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "        if model == \"srnn\":\n",
    "            cell = StackRNNCell(hidden_size, no_op = False,\n",
    "                                n_stack = n_stack, k = k)\n",
    "\n",
    "        model = RecurrentWrapper(cell, n_symbols = n_symbols, sgd_lr = sgd_lr)\n",
    "\n",
    "        sess = tf.Session()\n",
    "        sess.run([tf.global_variables_initializer(),\n",
    "                  tf.local_variables_initializer()])\n",
    "\n",
    "        n_max_train_ = 5\n",
    "        for i in range(n_iterations):\n",
    "            if i % 5 == 4:\n",
    "                n_max_train_ = min(n_max_train_ + 1, n_max_train)\n",
    "          \n",
    "            losses = []\n",
    "            for j in range(n_img_per_iter):\n",
    "                symbols_batch, targets_batch, _ = image_label[j],image_train[j]\n",
    "                current_loss, _ = sess.run(\n",
    "                    [model.loss, model.train_op],\n",
    "                    feed_dict = { model.symbols : symbols_batch,\n",
    "                                  model.targets : targets_batch })\n",
    "                losses.append(current_loss.mean())\n",
    "                \n",
    "            print(\"Number of iterations:\", i)\n",
    "            print(\" \", sum(losses) / len(losses))\n",
    "\n",
    "            # calculate accuracy for train + validation set\n",
    "            sequences_correct = []\n",
    "            for n in range(n_min, n_max_val + 1):\n",
    "                # is_pred signifies if next symbol is predictable\n",
    "                symbols_batch_, targets_batch_, is_pred_ = \\\n",
    "                                gen_.next_sequence()\n",
    "                \n",
    "                predictions = sess.run(model.preds,\n",
    "                    feed_dict = { model.symbols : symbols_batch_ })\n",
    "\n",
    "                correct = all([np.argmax(t[0]) == p[0] for t, p, i in \\\n",
    "                               zip(targets_batch_, predictions, is_pred_)\n",
    "                               if i is True])\n",
    "                sequences_correct.append(correct)\n",
    "            print(\" \", sum(sequences_correct) / len(sequences_correct))\n",
    "\n",
    "            # calculate accuracy for train + validation + test set\n",
    "            sequences_correct = []\n",
    "            for n in range(n_min, n_max_test + 1):\n",
    "                symbols_batch_, targets_batch_, is_pred_ = \\\n",
    "                                gen_test.next_sequence()\n",
    "                \n",
    "                predictions = sess.run(model.preds,\n",
    "                    feed_dict = { model.symbols : symbols_batch_ })\n",
    "\n",
    "                correct = all([np.argmax(t[0]) == p[0] for t, p, i in \\\n",
    "                               zip(targets_batch_, predictions, is_pred_)\n",
    "                               if i is True])\n",
    "                sequences_correct.append(correct)\n",
    "            print(\" \", sum(sequences_correct) / len(sequences_correct))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "NNFL.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
