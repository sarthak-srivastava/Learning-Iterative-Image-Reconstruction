{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StackRNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarthak-srivastava/Learning-Iterative-Image-Reconstruction/blob/master/StackRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Msf5Hj7v4r6I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def matmul_broadcast(a, b):\n",
        "    \"\"\"Multiply all innermost 2x2 matrices of a and b.\n",
        "       Args:\n",
        "         a: (a_n, ..., a_2, a_1) tensor\n",
        "         b: (b_n, ..., b_2, b_1) tensor, where a_1 = b_2\n",
        "       Returns:\n",
        "         (a_n, ..., a_3, b_n, ..., b_3, a_2, b_1) tensor\n",
        "    \"\"\"\n",
        "    # get shape as list\n",
        "    a_dims = [d if d != None else -1 for d in a.shape.as_list()]\n",
        "    b_dims = [d if d != None else -1 for d in b.shape.as_list()]\n",
        "    \n",
        "    # reshape a to (a_n * ... * a_2, a_1)\n",
        "    a_shape = np.array((-1, a_dims[-1]))\n",
        "    a_ = tf.reshape(a, a_shape)\n",
        "\n",
        "    # reshape b to (b_2, b_1 * b_n * ... * b_3)\n",
        "    b_shape = np.array((b_dims[-2], -1))\n",
        "    b_perm = [(i - 2 + len(b_dims)) % len(b_dims) for i in range(len(b_dims))]\n",
        "    b_ = tf.reshape(tf.transpose(b, perm = b_perm), b_shape)\n",
        "\n",
        "    # matrix product is (a_n * ... * a_2, b_1 * b_n * ... * b_3)\n",
        "    a_b = tf.matmul(a_, b_)\n",
        "\n",
        "    # refold to (a_n, ..., a_2, b_1, b_n, ..., b_3)\n",
        "    a_b_shape = np.array(a_dims[: -1] + b_dims[-1 :] + b_dims[: -2])\n",
        "    a_b = tf.reshape(a_b, a_b_shape)\n",
        "\n",
        "    # permute to (a_n, ..., a_3, b_n, ..., b_3, a_2, b_1)\n",
        "    a_b_perm = list(range(0, len(a.shape) - 2)) + \\\n",
        "               list(range(len(a.shape), len(a.shape) + len(b.shape) - 2)) + \\\n",
        "               list(range(len(a.shape) - 2, len(a.shape)))\n",
        "    a_b = tf.transpose(a_b, perm = a_b_perm)\n",
        "    \n",
        "    return a_b\n",
        "\n",
        "class StackRNNCell(tf.nn.rnn_cell.RNNCell):\n",
        "    \"\"\"StackRNN cell\n",
        "    Implementation is based on: https://arxiv.org/abs/1503.01007\n",
        "    \"\"\"\n",
        "    def __init__(self, num_units, no_op = False, n_stack = 1, k = 1,\n",
        "                 stack_size = 200, mode = 1, \n",
        "                 activation = None, reuse = None, name = None):\n",
        "        \"\"\"Initialize the Stack RNN cell.\n",
        "        Args:\n",
        "          num_units: int, The number of hidden units in the cell.\n",
        "          no_op: Bool, Whether to include no-op action. Default: False\n",
        "          n_stack: int, number of stacks. Default 1\n",
        "          k: int, number of items to read off the top of the stack. Default 1\n",
        "          stack_size: int, number of elements in stack. Default: 200\n",
        "          mode: int, switch between recurrence only through stacks (mode = 1)\n",
        "                     and recurrence through hidden layer + stacks (mode = 2)\n",
        "          activation: Activation function of inner states. Default: sigmoid\n",
        "          reuse: (optional) Bool, whether to use variables in an existing scope. \n",
        "          name: String, the name of the layer.\n",
        "        \"\"\"\n",
        "        super(StackRNNCell, self).__init__(_reuse = reuse, name = name)\n",
        "\n",
        "        # Inputs must be 2-dimensional.\n",
        "        self.input_spec = tf.contrib.keras.layers.InputSpec(ndim = 2)\n",
        "\n",
        "        self._num_units = num_units\n",
        "        self._no_op = no_op\n",
        "        self._n_stack = n_stack\n",
        "        self._k = k\n",
        "        self._stack_size = stack_size\n",
        "        self._mode = mode\n",
        "        self._activation = activation or tf.sigmoid\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return ([self._n_stack, self._stack_size + 1], [self._num_units])\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return self._num_units\n",
        "\n",
        "    def build(self, inputs_shape):\n",
        "        if inputs_shape[1].value is None:\n",
        "            raise ValueError(\"Expected inputs_shape[1] to be known\")\n",
        "        input_depth = inputs_shape[1].value\n",
        "\n",
        "        # hidden layer\n",
        "        self._U = self.add_variable(\"U\", shape = [input_depth, self._num_units])\n",
        "        self._R = self.add_variable(\"R\",\n",
        "            shape = [self._num_units, self._num_units])\n",
        "        self._P = self.add_variable(\"P\",\n",
        "            shape = [self._n_stack * self._k, self._num_units])\n",
        "        self._bias_hidden = self.add_variable(\"bias_hidden\",\n",
        "            shape = [self._num_units],\n",
        "            initializer = tf.zeros_initializer(dtype = self.dtype))\n",
        "\n",
        "        # action layer\n",
        "        n_actions = 3 if self._no_op else 2\n",
        "        self._A = self.add_variable(\"A\",\n",
        "            shape = [self._n_stack, self._num_units, n_actions])\n",
        "        self._bias_action = self.add_variable(\"bias_action\",\n",
        "            shape = [self._n_stack, n_actions],\n",
        "            initializer = tf.zeros_initializer(dtype = self.dtype))\n",
        "\n",
        "        # push to stack layer\n",
        "        self._D = self.add_variable(\"D\",\n",
        "            shape = [self._n_stack, self._num_units, 1])\n",
        "        self._bias_push = self.add_variable(\"bias_push\",\n",
        "            shape = [self._n_stack, 1],\n",
        "            initializer = tf.zeros_initializer(dtype = self.dtype))\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, state):\n",
        "        \"\"\"StackRNN cell.\n",
        "        Args:\n",
        "          inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n",
        "          state: Tuple of state tensors, first is tensor with shape\n",
        "                 `[batch_size, n_stack, stack_size]`, second is tensor\n",
        "                 with shape `[batch_size, state_size]`.\n",
        "        Returns:\n",
        "          A pair containing the new hidden state, and the new state. \n",
        "        \"\"\"\n",
        "        PUSH, POP, NO_OP = 0, 1, 2\n",
        "        s, h = state\n",
        "\n",
        "        # compute new hidden state\n",
        "        top_k = tf.reshape(s[:, :, 0 : self._k],\n",
        "                           (-1, self._n_stack * self._k))\n",
        "        h_in = [tf.matmul(inputs, self._U),\n",
        "                tf.matmul(top_k, self._P)]\n",
        "        if self._mode == 2:\n",
        "            h_in.append(tf.matmul(h, self._R))\n",
        "        new_h = self._activation(tf.add(tf.add_n(h_in), self._bias_hidden))\n",
        "\n",
        "        # compute weights of each stack action\n",
        "        swap_sb = lambda t: tf.transpose(t, perm = (1, 0, 2))\n",
        "        a = tf.nn.softmax(tf.add(swap_sb(matmul_broadcast(new_h, self._A)),\n",
        "                                 self._bias_action))\n",
        "\n",
        "        # compute values to push on top of the stack\n",
        "        d = self._activation(tf.add(swap_sb(matmul_broadcast(new_h, self._D)),\n",
        "                                    self._bias_push))\n",
        "        s_ = tf.concat((d, s), axis = 2)\n",
        "\n",
        "        # weighted average of each stack operation\n",
        "        ss = [tf.multiply(a[:, :, PUSH : PUSH + 1], s_[:, :, : -2]),\n",
        "              tf.multiply(a[:, :, POP : POP + 1], s_[:, :, 2 :])]\n",
        "        if self._no_op:\n",
        "            ss.append(tf.multiply(a[:, :, NO_OP : NO_OP + 1], s_[:, :, 1 : -1]))\n",
        "\n",
        "        # append sentinel value (-1) to bottom of stack\n",
        "        new_s = tf.concat((tf.add_n(ss), s[:, :, -1 :]), axis = 2)\n",
        "\n",
        "        return new_h, (new_s, new_h)\n",
        "\n",
        "    def zero_state(self, batch_size, dtype):\n",
        "        return (tf.zeros((batch_size, self._n_stack, self._stack_size + 1),\n",
        "                         dtype = dtype) - 1, \n",
        "                tf.zeros((batch_size, self._num_units), dtype = dtype))\n",
        "\n",
        "class StackLSTMCell(tf.nn.rnn_cell.RNNCell):\n",
        "    \"\"\"StackLSTM cell\n",
        "    Implementation is based on: https://arxiv.org/abs/1503.01007\n",
        "    \"\"\"\n",
        "    def __init__(self, num_units, no_op = False, n_stack = 1, k = 1,\n",
        "                 stack_size = 200, \n",
        "                 activation = None, reuse = None, name = None):\n",
        "        \"\"\"Initialize the Stack LSTM cell.\n",
        "        Args:\n",
        "          num_units: int, The number of hidden units in the cell.\n",
        "          no_op: Bool, Whether to include no-op action. Default: False\n",
        "          n_stack: int, number of stacks. Default 1\n",
        "          k: int, number of items to read off the top of the stack. Default 1\n",
        "          stack_size: int, number of elements in stack. Default: 200\n",
        "          activation: Activation function of inner states. Default: sigmoid\n",
        "          reuse: (optional) Bool, whether to use variables in an existing scope. \n",
        "          name: String, the name of the layer.\n",
        "        \"\"\"\n",
        "        super(StackLSTMCell, self).__init__(_reuse = reuse, name = name)\n",
        "\n",
        "        # Inputs must be 2-dimensional.\n",
        "        self.input_spec = tf.contrib.keras.layers.InputSpec(ndim = 2)\n",
        "\n",
        "        self._num_units = num_units\n",
        "        self._no_op = no_op\n",
        "        self._n_stack = n_stack\n",
        "        self._k = k\n",
        "        self._stack_size = stack_size\n",
        "        self._activation = activation or tf.sigmoid\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return ([self._n_stack, self._stack_size + 1],\n",
        "                [self._num_units], [self._num_units])\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return self._num_units\n",
        "\n",
        "    def build(self, inputs_shape):\n",
        "        if inputs_shape[1].value is None:\n",
        "            raise ValueError(\"Expected inputs_shape[1] to be known\")\n",
        "        input_depth = inputs_shape[1].value\n",
        "        init = tf.zeros_initializer(dtype = self.dtype)\n",
        "\n",
        "        # lstm layer\n",
        "        self._R = self.add_variable(\"R\",\n",
        "            shape = [input_depth + self._num_units, 4 * self._num_units])\n",
        "        self._bias_lstm = self.add_variable(\"bias_lstm\",\n",
        "            shape = [4 * self._num_units],\n",
        "            initializer = init)\n",
        "\n",
        "        # hidden layer\n",
        "        self._P = self.add_variable(\"P\",\n",
        "            shape = [self._n_stack * self._k, self._num_units])\n",
        "        self._bias_hidden = self.add_variable(\"bias_hidden\",\n",
        "            shape = [self._num_units],\n",
        "            initializer = init)\n",
        "\n",
        "        # action layer\n",
        "        n_actions = 3 if self._no_op else 2\n",
        "        self._A = self.add_variable(\"A\",\n",
        "            shape = [self._n_stack, self._num_units, n_actions])\n",
        "        self._bias_action = self.add_variable(\"bias_action\",\n",
        "            shape = [self._n_stack, n_actions],\n",
        "            initializer = init)\n",
        "\n",
        "        # push to stack layer\n",
        "        self._D = self.add_variable(\"D\",\n",
        "            shape = [self._n_stack, self._num_units, 1])\n",
        "        self._bias_push = self.add_variable(\"bias_push\",\n",
        "            shape = [self._n_stack, 1],\n",
        "            initializer = init)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, state):\n",
        "        \"\"\"StackLSTM cell.\n",
        "        Args:\n",
        "          inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n",
        "          state: Tuple of state tensors, first is tensor with shape\n",
        "                 `[batch_size, n_stack, stack_size]`, second and third are\n",
        "                 tensors with shape `[batch_size, state_size]`.\n",
        "        Returns:\n",
        "          A pair containing the new hidden state, and the new state. \n",
        "        \"\"\"\n",
        "        PUSH, POP, NO_OP = 0, 1, 2\n",
        "        s, c, h = state\n",
        "\n",
        "        # compute lstm state\n",
        "        gate_in = tf.add(tf.matmul(tf.concat([inputs, h], axis = 1), self._R),\n",
        "                         self._bias_lstm)\n",
        "\n",
        "        # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
        "        i, j, f, o = tf.split(\n",
        "            value = gate_in, num_or_size_splits = 4, axis = 1)\n",
        "\n",
        "        forget_bias = tf.constant(1, dtype = f.dtype)\n",
        "        new_c = tf.add(tf.multiply(c, tf.sigmoid(tf.add(f, forget_bias))),\n",
        "                       tf.multiply(tf.sigmoid(i), self._activation(j)))\n",
        "        lstm_h = tf.multiply(self._activation(new_c), tf.sigmoid(o))\n",
        "\n",
        "        # compute new hidden state\n",
        "        top_k = tf.reshape(s[:, :, 0 : self._k],\n",
        "                           (-1, self._n_stack * self._k))\n",
        "        new_h = self._activation(tf.add(tf.add(tf.matmul(top_k, self._P),\n",
        "                                               lstm_h),\n",
        "                                        self._bias_hidden))\n",
        "\n",
        "        # compute weights of each stack action\n",
        "        swap_sb = lambda t: tf.transpose(t, perm = (1, 0, 2))\n",
        "        a = tf.nn.softmax(tf.add(swap_sb(matmul_broadcast(new_h, self._A)),\n",
        "                                 self._bias_action))\n",
        "\n",
        "        # compute values to push on top of the stack\n",
        "        d = self._activation(tf.add(swap_sb(matmul_broadcast(new_h, self._D)),\n",
        "                                    self._bias_push))\n",
        "        s_ = tf.concat((d, s), axis = 2)\n",
        "\n",
        "        # weighted average of each stack operation\n",
        "        ss = [tf.multiply(a[:, :, PUSH : PUSH + 1], s_[:, :, : -2]),\n",
        "              tf.multiply(a[:, :, POP : POP + 1], s_[:, :, 2 :])]\n",
        "        if self._no_op:\n",
        "            ss.append(tf.multiply(a[:, :, NO_OP : NO_OP + 1], s_[:, :, 1 : -1]))\n",
        "\n",
        "        # append sentinel value (-1) to bottom of stack\n",
        "        new_s = tf.concat((tf.add_n(ss), s[:, :, -1 :]), axis = 2)\n",
        "\n",
        "        return new_h, (new_s, new_c, new_h)\n",
        "\n",
        "    def zero_state(self, batch_size, dtype):\n",
        "        return (tf.zeros((batch_size, self._n_stack, self._stack_size + 1),\n",
        "                         dtype = dtype) - 1, \n",
        "                tf.zeros((batch_size, self._num_units), dtype = dtype),\n",
        "                tf.zeros((batch_size, self._num_units), dtype = dtype))\n",
        "\n",
        "class RecurrentWrapper:\n",
        "    def __init__(self, cell, n_symbols = 2, sgd_lr = 0.01, hard_clip = 15.0):\n",
        "        \"\"\"n_symbols: number of output symbols\n",
        "           sgd_lr: learning rate\n",
        "           hard_clip: maximum absolute value of gradients\n",
        "        \"\"\"\n",
        "        # Placeholder for the inputs in a given iteration (batch_size = 1)\n",
        "        self.symbols = tf.placeholder(tf.float32, [None, None, n_symbols],\n",
        "                                      name = \"lstm_symbols\")\n",
        "        self.targets = tf.placeholder(tf.float32, [None, None, n_symbols],\n",
        "                                      name = \"lstm_targets\")        \n",
        "\n",
        "        # Initial state of the LSTM memory\n",
        "        batch_size = tf.shape(self.symbols)[1]\n",
        "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "\n",
        "        # Given inputs with shape (time, batch, input_size) outputs:\n",
        "        #  - outputs: (time, batch, output_size)\n",
        "        #  - states:  (time, batch, hidden_size)\n",
        "        outputs, states = tf.nn.dynamic_rnn(cell, self.symbols,\n",
        "                                            initial_state = self.initial_state,\n",
        "                                            time_major = True)\n",
        "\n",
        "        # add linear layer\n",
        "        final_projection = lambda x: tf.contrib.layers.fully_connected(\n",
        "            x, num_outputs = n_symbols, activation_fn = None)\n",
        "\n",
        "        self.outputs = outputs = tf.map_fn(final_projection, outputs)\n",
        "\n",
        "        # predicted symbol is symbol with max probability\n",
        "        self.probs = tf.nn.softmax(self.outputs)\n",
        "        self.preds = tf.argmax(self.outputs, axis = 2)\n",
        "\n",
        "        # loss and optimization\n",
        "        self.loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "            labels = self.targets, logits = outputs, name = \"cross_entropy\")\n",
        "\n",
        "        clip = lambda grad: tf.clip_by_value(grad, -hard_clip, +hard_clip) \\\n",
        "               if grad is not None else grad\n",
        "        opt = tf.train.GradientDescentOptimizer(sgd_lr)\n",
        "        gvs = opt.compute_gradients(self.loss)\n",
        "        gvs_ = [(clip(grad), var) for grad, var in gvs]\n",
        "        self.train_op = opt.apply_gradients(gvs_)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}